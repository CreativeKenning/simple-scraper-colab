{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "H0DvidVtxrgq"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CreativeKenning/simple-scraper-colab/blob/main/Scraper_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## How to make your own copy\n",
        "\n",
        "In order to run this scraper, you'll need to make your own copy, to do so:\n",
        "1. Navigate to the \"file\" tab\n",
        "2. Click \"Save a Copy in Drive\"\n",
        "3. You're set! Start working with that copy\n",
        "\n",
        "## Some Colab Tips\n",
        "\n",
        "1. If you'd like to hide the \"form\" overlaying the code, simply right click on a cell, find the \"form\" tab, and click \"hide form\". I've worked to comment all code to the best of my ability for those interested in learning how the scraper works.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4gHYb4zcyM26"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What Is the Purpose of this Notebook?\n",
        "1. Introduce you to the basic structure of HTML\n",
        "2. Demonstrate how to investigate HTML using BeautifulSoup\n",
        "2. Guide you through scraping in a no coding environment with simple data-export in CSV and TXT files for easy analysis using tools like [Orange](https://orangedatamining.com/) or [Antconc](https://www.laurenceanthony.net/software/antconc/)\n",
        "3. Provide Multi-variable scraping and Data-export options\n",
        "4. Provide a simplified web-crawler that pulls all links from a given website, and exports them as a csv or txt file."
      ],
      "metadata": {
        "id": "oEpxVUE2T4Fa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Prologue: Importing Libraries and Making the Soup\n",
        "* 1. [Library Import (Mandatory!)](#scrollTo=eXLTS2V64ztn&line=1&uniqifier=1)\n",
        "  * Imports Necessary Libraries to run the code\n",
        "* 2. [Choose Website](#scrollTo=yOpxFGXFlYpG&line=1&uniqifier=1)\n",
        "  * Enter the URL of the website you want to scrape\n",
        "* 3.  [Making the Soup](#scrollTo=Dsi-PhX7h2wT&line=1&uniqifier=1)\n",
        "  * Ensure we can pull the raw HTML from the chosen website\n",
        "\n",
        "## Section A. Fltering HTML and Simple 2 tag scraping\n",
        "\n",
        "* 1A. [Prettified HTML Viewing](#scrollTo=A0JhvWZFmBPk&line=1&uniqifier=1)\n",
        "  * View chosen websites' HTML in a clean, hierarchical manner\n",
        "* 2A. [HTML Filtering By Tag and Class](#scrollTo=kmP6VUMLWp6l&line=1&uniqifier=1)\n",
        "  * Interactive HTML filter: find the data you want to scrape\n",
        "* 3A. [Simple 2-tag scraping](#scrollTo=5VBqh_qjYnaf&line=9&uniqifier=1)\n",
        "  * Simple Scraper: Cell to pull and view data from up to two HTML tag/class pairs\n",
        "* 4A. [Data Export](#scrollTo=7mx6qoosxMH_&line=1&uniqifier=1)\n",
        "  * Export your data in TXT or CSV format\n",
        "\n",
        "### Section B. Multi-Tag Scraping and export\n",
        "* 1B. [Multi-Tag input](#scrollTo=aiJfr2MAdISv&line=1&uniqifier=1)\n",
        "  * A multi-tag input for those looking to scrape more than two HTML tag/class pairs\n",
        "* 2B. [Dictionary Checker](#scrollTo=IpB_q-8xrsQ9)\n",
        "  * A proofing cell to check your inputs to ensure there aren't any typos\n",
        "* 3B. [Multi-Tag Scraper](#scrollTo=kJ_YuM4msne5&line=1&uniqifier=1)\n",
        "  * Running the multi-tag scraper and viewing the scraped data\n",
        "* 4b. [Multi-Tag Data Export](#scrollTo=6Qh9Or8uPpTm&line=3&uniqifier=1)\n",
        "  * Data Export in TXT or CSV format\n",
        "  \n",
        "### Section C. Webpage crawling and overview tool\n",
        "* 1C. [Website picker and Crawler](#scrollTo=i5cECfS2-u7K&line=1&uniqifier=1)\n",
        "  * A simple website crawler that extracts all webpages from your website, and displays a simple overview of their \"paragraph\" tags\n",
        "* 2C.[Website Crawler Data Export](#scrollTo=WwGWyuSx487r&line=1&uniqifier=1)\n",
        "  * Data Export for your crawled website as TXT or CSV file\n",
        "\n"
      ],
      "metadata": {
        "id": "uf-77U0onCNg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prologue: Importing Libraries, Entering your Website\n"
      ],
      "metadata": {
        "id": "XihGM0Xsu0XY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXLTS2V64ztn",
        "collapsed": true,
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown ### 1. Importing libraries\n",
        "\n",
        "#@markdown First things first, run this cell to import all the libraries this notebook reqiures to function.\n",
        "#@markdown <br><br>\n",
        "#@markdown This also mounts to your\n",
        "#@markdown GoogleDrive. If you would like to save the data gathered fropm this Colab sheet you should allow access to your drive when prompted\n",
        "\n",
        "#@markdown You only need to run this once per session.\n",
        "!pip install Colorama\n",
        "!pip install ipywidgets\n",
        "!pip install regex\n",
        "import regex as re\n",
        "import ipywidgets as widgets\n",
        "import colorama\n",
        "import ast\n",
        "from colorama import Fore, Back, Style\n",
        "from pathlib import Path\n",
        "from bs4 import SoupStrainer, BeautifulSoup\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import nltk.data\n",
        "import urllib\n",
        "from urllib import request\n",
        "from urllib.parse import urljoin, urlparse\n",
        "import urllib.request as ur\n",
        "import requests\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### 2. Choosing Your website\n",
        "\n",
        "#@markdown Run this cell, and paste the URL of the website you want to scrape and hit Enter.\n",
        "#@markdown * If you're looking for an easy website to get started, I recommend https://quotes.toscrape.com\n",
        "#@markdown I will be using this website as an example throughout\n",
        "#@markdown * If you want to scrape multiple webpages, come back and run this again with a new website\n",
        "#@markdown\n",
        "\n",
        "\n",
        "url = input(\"Enter Your URL and hit Enter to get started!\").strip()\n",
        "\n",
        "print(Fore.GREEN +\"Okay, you've entered \" + url)"
      ],
      "metadata": {
        "id": "yOpxFGXFlYpG",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### 3. Making the BeautifulSoup Object\n",
        "\n",
        "#@markdown 1. Run this cell, and it will attempt to pull the HTML from your chosen website\n",
        "#@markdown <br><br>This cell just makes sure we can pull the website, we'll parse the HTML in the next cell.\n",
        "\n",
        "\n",
        "#This block essentially pulls the the HTML from the website using the *requests* library.\n",
        "#Then, we use beautiful soup to turn that website into a \"beautiful soup object\" called \"soup\".\n",
        "#@markdown\n",
        "\n",
        "\n",
        "# Send a GET request to the website\n",
        "response = requests.get(url)\n",
        "\n",
        "# Create a BeautifulSoup object and parse with an HTML parser\n",
        "soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "# If the status code of the \"resopnse\" object is 200 (meaning it's all good)\n",
        "if response.status_code == 200:\n",
        "  #Print this message with the color red\n",
        "  print(Fore.RED + f\"We've created our beautifulSoup object, represented by the variable 'soup' \")\n",
        "#If the status code is 404 (meaning it isn't found)\n",
        "elif response.status_code == 404:\n",
        "  #Let the user know something went wrong\n",
        "  print(Fore.RED +f\"Something went wrong: please doublecheck your URL and try again\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Dsi-PhX7h2wT",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section A: Two-tag Scraper"
      ],
      "metadata": {
        "id": "D36K_Tw3eToE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**1A. Viewing the HTML**\n",
        "\n",
        "HTML stands for HyperText-Markup-Language, and HTML documents are used to format websites. Because HTML provides an internally-consistant structure for a webpage, we can use HTML tags (the structures that tell browsers how to display information) to collect the data we want to view. For example, the following\n",
        "\n",
        "```\n",
        "<div> X </div>\n",
        "```\n",
        "indicates that \"X\" will be formatted by the `<div></div>` tag. The first half `<div>` opens the tag, while the latter half` </div>` closes the tag\n",
        "<br><br>\n",
        "\n",
        "HTML tags are hierarchical, meaning higher-level tags will contain the tags below it.Tags that contain other tags are called 'parent' tags, the tags contained within a parent are called  'child' tags. Tags that share a parent are called 'Sibling' tags.\n",
        "<br><br>\n",
        "The following cell helps you find what tags contain the data you want to scrape, by filtering out the 'noise' and formatting tags hierarchically.\n",
        "<br><br>\n",
        " We will also be looking for the 'class' of the parent and child tags, if any: they allow the same tags (such as `<p></p>`, a paragraph tag), to be styled differently across the webpage using CSS (Cascading Style Sheets). For example:\n",
        "\n",
        "```\n",
        "<h1 class=\"bold\"> This will be styled as a bold heading </h1>\n",
        "  <p class=\"author\"> this paragraph will be styled as \"author\"</p>\n",
        "  <p class=\"body\"> while this paragraph will be styled as \"body\"</p>\n",
        "  <p class=\"subscript\"> in this example, <h1></h1> is the \"parent\" tag,\n",
        "  while there are three sibling \"<p></p>\" tags,each with a different class </p>\n",
        "```\n",
        "\n",
        "By identifying the HTML tag and class formatting the data we want to extract, we can efficiently extract data from webpages, while minimizing the 'noise' (extraneous data) that needs to be cleaned out.\n",
        "\n",
        "<br><br>\n",
        "\n",
        " As you explore the HTML of your  chosen webpage,\n",
        "\n",
        " 1. look for the tags that contain the data you want(child tags)\n",
        " 2. Look for the tags directly above them (parent tags)\n",
        " 3. Note any tags in the HTML tree that are adjacent to your data (sibling tags)\n",
        " 4. Note the HTML 'class' if any, of all parent, child, and sibling tags\n",
        "\n",
        "This information will not only help you pull the data you need, but filter the noise that you don't! You can view the HTML in the notebook itself, or export it and view in notepad or your application of choice.\n"
      ],
      "metadata": {
        "id": "tQZ43EompJK4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### 2A. Viewing the HTML\n",
        "#@markdown Run this cell to view the HTML!\n",
        "\n",
        "#soup.prettify() cleans and displays HTML in an easy to read manner\n",
        "x = soup.prettify()\n",
        "\n",
        "# Ask the user if they want to export the content as a text file or view it in the browser\n",
        "user_input = input(\"Would you like to export this as a text file or view in browser? Please enter Y to save as a .txt file, or N to view in browser: \").upper()\n",
        "\n",
        "if user_input == \"N\":\n",
        "    print(x)\n",
        "elif user_input == \"Y\":\n",
        "    # Ask for the filename\n",
        "    filename = input(\"Please enter the file name, please follow best practices e.g. CamelCase: \") +\".txt\"\n",
        "\n",
        "    # Define the file path in Google Drive\n",
        "    file_path = Path(\"/content/drive/My Drive/scraper\") / filename\n",
        "\n",
        "    file_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    # Write the prettified HTML content to the file in Google Drive\n",
        "    with open(file_path, \"w\") as file_object:\n",
        "        file_object.write(x)\n",
        "\n",
        "    print(f\"The content has been exported to {file_path}\")\n",
        "else:\n",
        "    print(\"I couldn't understand that, please enter Y or N\")\n",
        "\n"
      ],
      "metadata": {
        "id": "A0JhvWZFmBPk",
        "collapsed": true,
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### 3A. Filtering cell: Find the parent and child tags you need for analysis\n",
        "#@markdown By now, you should have a general idea of where the data you need is located in the HTML tree; this cell helps us refine that further.\n",
        "\n",
        "#@markdown <br> 1. Enter the parent tag you want to search under in the 'findtag' section,\n",
        "#@markdown and its class (if any) in the 'findclass' box\n",
        "\n",
        "#@markdown <br><br> 2. Run this cell to investigate the 'children' contained beneath the parent tag/class pair you entered\n",
        "\n",
        "findtag = 'div' #@param {type:\"string\"}\n",
        "#@markdown Filter the tags by CSS class\n",
        "findclass = 'col-md-8' #@param {type:\"string\"}\n",
        "\n",
        "# @markdown If you don't know where to begin, try\n",
        "#@markdown checking out [quotestoscrape.com](https://quotes.toscrape.com/),\n",
        "#@markdown note how the same HTML tag, \"div\", contains\n",
        "#@markdown different information based on its class. E.g. the 'quote' class<br><br>\n",
        "#@markdown Try entering 'div' in the box findtag, and the class 'quote', 'row', 'col-md-8', or 'col-md-4', in the 'findclass' box.\n",
        "#@markdown <br><br> Notice how you can filter for different classes of information by changing the tag our scraper looks for.\n",
        "#@markdown <br><br> Further, notice how we can filter for specific data contained within \"div\" by changing the class our scraper searches for.\n",
        "#@markdown This is important for ensuring you don't pull any extraneous data contained within\n",
        "#@markdown siblings of your chosen parent tag, that might make cleaning more difficult!\n",
        "#@markdown <br><br> Once you've refined your search,\n",
        "#@markdown make a note of the *tag* and *class* of any child tags\n",
        "#@markdown containing important information that you want to scrape, and the tag and class of their *direct parent tag*\n",
        "#@markdown We'll be utilizing that information in the next cell<br>\n",
        "\n",
        "if findclass:  # This begins an if/elif/else statement that checks *if* the variable 'findall' has a value\n",
        "    z = soup.find_all(findtag, class_=findclass) #If it does, then the code searches for user-defined tag and class\n",
        "else: #if findclass has no value\n",
        "    z = soup.find_all(findtag) #Then the code only searches for the tag\n",
        "\n",
        "if z: #further, if \"z\"--the filtered HTML tag\" has a value\n",
        "    for tag in z: #then for each tag in the filtered HTML Z\n",
        "        print(tag.prettify()) #print the prettified version\n",
        "else: #if z has no value\n",
        "    print(f\"No elements found for tag: {findtag} with class: {findclass}\") #Then print that no elements have been found\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kmP6VUMLWp6l",
        "collapsed": true,
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tag_scrape(entry,tag, _class):\n",
        "  if entry.find(tag, {\"class\": _class}): #This code is based on the previous entry, the only difference is the changed class \"event-box-date\"\n",
        "      description = entry.find(tag,{\"class\": _class}).get_text().strip()\n",
        "  else:\n",
        "    description = \"none\"\n",
        "  return(description)\n",
        "\n",
        "\n",
        "#@markdown ### 4A: Two-Tag Scraping cell\n",
        "#@markdown In this cell, you will take the HTML tags you've identified in 3A and enter them here; now that we know *exactly* where you're data is, we're going to export it to your google drive.\n",
        "#@markdown <br><br>\n",
        "#@markdown How to use this cell:\n",
        "#@markdown 1. First, enter the parent tag and class you identified in section 3A: enter the parent tag and its class into the \"findall\" box, and \"findclass\" box<br><br>\n",
        "#@markdown 1a. If you are using https://quotes.toscrape.com, try using \"div\" as your parent tag, and \"quote\" as its' class\n",
        "\n",
        "#@markdown 2. Then, take the child tag(s) you identified in 3A, and enter them into tag1 and class1, and tag2 and class2 respectively. <br>\n",
        "#@markdown 2a. For those using https://quotes.toscrape.com, try entering \"span\" and \"text\" as the first tag/class pair, and \"small\" and \"author\" as the second\n",
        "#@markdown <br>2b. If you only have one tag worth of data you want to collect, leave tag2 and class2 blank <br><br>\n",
        "\n",
        "#@markdown 3. Enter the column names for the data you're collecting in \"namedata1\" and \"namedata2\" if applicable; this tells the code what to name the columns in your data.<br><br>\n",
        "#@markdown 4. Finally, run the cell to collect your data. If you don't pull any data, first check your spelling. Then, go back to 3A, and make sure that you correctly identified the parent tags *above* your data, and the child tags that *contain* your data.\n",
        "\n",
        "\n",
        "#@markdown Filtering: Enter the parent tag and class you want to look beneath:\n",
        "findall = \"div\" #@param  {type:\"string\"}\n",
        "findclass = \"quote\" #@param {type:\"string\"}\n",
        "#@markdown <br>\n",
        "#@markdown Enter the data-containing first child tag and class pair here\n",
        "tag1 = \"span\" #@param  {type:\"string\"}\n",
        "class1 = \"text\" #@param {type:\"string\"}\n",
        "# @markdown Enter your second child tag and class pair here (optional)\n",
        "tag2 = \"small\" #@param {type:\"string\"}\n",
        "class2 = \"author\" #@param {type:\"string\"}\n",
        "#@markdown What kind of data are you scraping? Provide column names here\n",
        "namedata1 = \"text\" #@param {type:\"string\"}\n",
        "namedata2 = \"author\" #@param {type:\"string\"}\n",
        "#@markdown <br>\n",
        "\n",
        "# @markdown If everything goes well, you should see your data in a \"dataframe\" at the bottom of this cell.\n",
        "# @markdown A dataframe is a powerful data structure you can use in the process of storing, cleaning, and analyzing data.\n",
        "# @markdown <br><br>\n",
        "# @markdown Plus, colab makes it easy by integrating auto-coding graphs and options directly into its DF user interface.\n",
        "# @markdown <br><br>\n",
        "# @markdown Try clicking the \"convert dataframe into interactive table\" or \"suggest charts\" button to find some new ways to view the data you just collected.\n",
        "# @markdown\n",
        "# @markdown Alternatively, you can export it to a CSV file in the next cell\n",
        "if findclass:  # This begins an if/elif/else statement that checks *if* the variable 'findall' has a value\n",
        "  entries = soup.find_all(findall, class_=findclass) #If it does, then the code searches for user-defined tag and class\n",
        "else: #if findclass has no value\n",
        "    entries = soup.find_all(findall) #Then the code only searches for the tag\n",
        "\n",
        "data = []# creates a list titled \"data\"\n",
        "\n",
        "for entry in entries:\n",
        "  if tag2:\n",
        "    name = tag_scrape(entry,tag1,class1)\n",
        "    description = tag_scrape(entry,tag2,class2)\n",
        "    data.append({namedata1: name, namedata2: description})\n",
        "  else:\n",
        "    name = tag_scrape(entry,tag1,class1)\n",
        "    data.append({namedata1: name})\n",
        "\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "\n",
        "df\n"
      ],
      "metadata": {
        "id": "5VBqh_qjYnaf",
        "collapsed": true,
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ## 3A. Data Export\n",
        "#@markdown Run this cell and follow its prompts to save your scraped data in a google drive folder\n",
        "def saveFile(fileName, dataframe):\n",
        "  file_path = Path(\"/content/drive/My Drive/scraper\") / fileName\n",
        "  file_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "  df.to_csv(file_path, index=False)\n",
        "  return file_path\n",
        "\n",
        "print(Fore.RED + \"Would you like to export your data as a CSV or TXT file?\")\n",
        "user_input = input(f\"Please enter '+' to save the whole dataframe, or '-' if you wouldlike to choose specific columns\")\n",
        "\n",
        "\n",
        "if user_input == \"+\":\n",
        "  fileChoice = input(\"would you like to save this file as a CSV or a txt file? Please input c for CSV or t for txt\").lower()\n",
        "  urFile = input(\"Now please enter the name of the file\")\n",
        "  if fileChoice == \"c\":\n",
        "    file = urFile+\".csv\"\n",
        "    file_path = saveFile(file,df)\n",
        "    print(Fore.GREEN + f\"Success! Check your drive for your CSV file for {file_path}\")\n",
        "  elif fileChoice == \"t\":\n",
        "    file = urFile+\".txt\"\n",
        "    file_path = saveFile(file,df)\n",
        "    print(Fore.GREEN + f\"Success! Check your drive for your TXT file for {file_path}\")\n",
        "  else :\n",
        "    print(\"please enter c or t\")\n",
        "\n",
        "elif user_input == \"-\":\n",
        "  columnInput = input(\"Please enter the column names you want to save, seperated by a comma\")\n",
        "  names = columnInput.split(',')\n",
        "  names = [name.strip().lower() for name in names]\n",
        "  df = df[names]\n",
        "  fileChoice = input(\"would you like to save this file as a CSV or a txt file? Please input c for CSV or t for txt\").lower()\n",
        "  urFile = input(\"Now please enter the name of the file\")\n",
        "\n",
        "  if fileChoice == \"c\":\n",
        "    file = urFile+\".csv\"\n",
        "    file_path = saveFile(file,df)\n",
        "    print(Fore.GREEN + f\"Success! Check your drive for your CSV file at {file_path}\")\n",
        "\n",
        "  elif fileChoice == \"t\":\n",
        "    file = urFile+\".txt\"\n",
        "    file_path = saveFile(file,df)\n",
        "    print(Fore.GREEN + f\"Success! Check your drive for your TXT file at {file_path}\")\n",
        "  else :\n",
        "    print(\"please enter c or t\")\n",
        "else:\n",
        "  print('please enter + or -')\n"
      ],
      "metadata": {
        "id": "7mx6qoosxMH_",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1a13be1-f61f-40cf-911a-f7e7ea60e912"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mWould you like to export your data as a CSV or TXT file?\n",
            "Please enter '+' to save the whole dataframe, or '-' if you wouldlike to choose specific columns+\n",
            "would you like to save this file as a CSV or a txt file? Please input c for CSV or t for txtT\n",
            "Now please enter the name of the filebigoTest\n",
            "\u001b[32mSuccess! Check your drive for your TXT file for /content/drive/My Drive/scraper/bigoTest.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section B: Multi-Tag Scraper"
      ],
      "metadata": {
        "id": "peYmwLa4eZpi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following cells are designed so you can scrape as many tag-class pairs as you want, and export the data in your chosen format.\n",
        "\n",
        "### Step 1: Input\n",
        "\n",
        "Make a note of your tag-class pairs beforehand, and decide on how you want to label the data. You will enter the tag, class, and desired data-label into the \"input\" cell. This cell turns your parameters into a \"list of dictionaries\" which tell the scraper what tags to pull, and how to sort the resulting data\n",
        "\n",
        "### Step 2: Double Check and Processing\n",
        "Be sure to double check your tag/class pairs and column names by running the \"double check\" cell. This will ensure you collect the correct data, and also will demonstrate how Python stores your data in a dictionary in key-value pairs: in this instance \"tag\",\"class\", and \"column\" are the keys, while you enter the values that tell the cell what to scrape [You can read up on dictionaries here](https://www.codecademy.com/resources/docs/python/dictionaries).\n",
        "\n",
        "## Step 3: Run the Processing cell\n",
        "Once you've double checked your tag/class pairs, run the \"processing\" cell, and view the resulting dataframe for patterns, or to see if the correct data was pulled. This may take a couple tries, with or without typos. I recommend writing your tags down on a piece of scratch paper to keep things straight,or in a handy txt file in another tab.\n",
        "\n",
        "### Step 4: Data Export\n",
        "\n",
        "After inspecting the dataframe to ensure everything is in order, run the data export cell to choose how you want to export your data. You can choose a specific file type (CSV or TXT) and specific columns.\n",
        "\n"
      ],
      "metadata": {
        "id": "R-McSQ7er7oW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title B1: Multi Tag-Class Scraper: Input\n",
        "\n",
        "#@markdown This cell is specifically designed for if you have more than two tags you need to scrape\n",
        "\n",
        "#@markdown * Run this cell, and enter the tag, class, and desired column name for your desired data\n",
        "#@markdown * Be careful: if you click \"add\" and have a typo, you'll have to run this cell again\n",
        "#@markdown *The widget is not self-refreshing, so you'll need to delete and re-enter the next tag/value/column entry each time you add a new one\n",
        "#@markdown * If you want to use http://quotes.toscrape.com as an example:\n",
        "#@markdown try entering: <br> <br>HTML tag: span. CSS class: text, Column Name: text\n",
        "#@markdown <br>HTML tag: small. CSS Class: author. Column Name: author.<br>\n",
        "#@markdown HTML tag: a. CSS Class: tag, Column Name: tag\n",
        "def display_data():\n",
        "    display(widgets.Label(value='Current tag-class-column pairs:'))\n",
        "    for pair in tag_class_pairs:\n",
        "        display(widgets.Label(value=str(pair)))\n",
        "def on_add_button_clicked(b):\n",
        "    tag_class_pairs.append({\n",
        "      'tag': tag_input.value,\n",
        "      'class': class_input.value,\n",
        "      'column': column_input.value\n",
        "    })\n",
        "    display_data()\n",
        "\n",
        "# Interactive form for user input\n",
        "tag_input = widgets.Text(description='HTML Tag:')\n",
        "class_input = widgets.Text(description='CSS Class:')\n",
        "column_input = widgets.Text(description='Column Name:')\n",
        "add_button = widgets.Button(description='Add')\n",
        "\n",
        "tag_class_pairs = []\n",
        "\n",
        "\n",
        "\n",
        "add_button.on_click(on_add_button_clicked)\n",
        "\n",
        "\n",
        "display(tag_input, class_input, column_input, add_button)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aiJfr2MAdISv",
        "collapsed": true,
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kBwlTqW2wDT-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title B2: Double-check your tag class pairs and column titles\n",
        "#@markdown Run this cell to see the tag, class, and column name for your data.\n",
        "#@markdown <br> <br>It should look like this \"[{'tag': 'your value', 'class': 'your value', 'column': 'your column name'}]\" and etc...\n",
        "\n",
        "print(tag_class_pairs)"
      ],
      "metadata": {
        "id": "IpB_q-8xrsQ9",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title B3: Multi-Tag Scraper: Processing\n",
        "\n",
        "#@markdown # How to use this cell:\n",
        "#@markdown 1. Ensure that you've determined the tag and class of the parent tag whose children you need to pull, and enter them into the form below\n",
        "#@markdown 2. Ensure that you've double checked your tag/class pairs in the dictionary-viewer above.\n",
        "#@markdown 3. Hit the \"run cell\" button and view the resulting dataframe.\n",
        "#@markdown 4. Note the names of any columns you want to export, or simply export the whole dataframe using the following cell.\n",
        "#@markdown 5. If you are using http://quotes.toscrape.com/ as a tutorial, try entering \"div\" as the parent tag, and \"quote\" as the class\n",
        "\n",
        "\n",
        "#@markdown If you encounter errors, double check the parent tag and class entered in the \"findall\" and \"findclass\" boxes.\n",
        "#@markdown <br><br>Next, ensure that the tag/class pairs you've entered correspond to the data you need to pull in the HTML.\n",
        "#@markdown <br><br>Try running this with one tag first, and slowly adding more if you continue having errors.\n",
        "#@markdown <br><br>\n",
        "\n",
        "\n",
        "findall1 = \"div\" #@param  {type:\"string\"}\n",
        "findclass1 = \"quote\" #@param {type:\"string\"}\n",
        "#@markdown <br>\n",
        "\n",
        "#tag_class_pairs = [{'tag':'span', 'class':'text', 'column':'text'}, {'tag':'small', 'class':'author', 'column':'author'},{'tag':'small','class':'author', 'column':'author'},{'tag':'a', 'class':'tag', 'column':'tags'}]\n",
        "\n",
        "def tag_scrape(entry, tag, _class):\n",
        "    elements = entry.find_all(tag, {\"class\": _class})\n",
        "    if elements:\n",
        "        return [element.get_text().strip() for element in elements]\n",
        "    else:\n",
        "        return [\"none\"]\n",
        "\n",
        "if findclass1:  # This begins an if/elif/else statement that checks *if* the variable 'findall' has a value\n",
        "  entries = soup.find_all(findall1, class_=findclass1) #If it does, then the code searches for user-defined tag and class\n",
        "else: #if findclass has no value\n",
        "  entries = soup.find_all(findall1) #Then the code only searches for the tag\n",
        "\n",
        "data = []#Creates an empty dictionary we will use to hold our data\n",
        "\n",
        "\n",
        "##To modify what gets scraped.change the value of each \"key:value\" pair; to collect more, copy and paste the {dictionary} within the [list] and edit accordingly\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for entry in entries: #for each entry in the soup object (entries)\n",
        "  entry_data = {} #first open a dictionary titled \"entry data\"\n",
        "  for pair in tag_class_pairs: #Then for each entry in the list of dictionaries \"tag_class_pairs\"\n",
        "    tag = pair[\"tag\"]# \"Tag\" is equal to the pair of the \"tag\" key in the dictionary\n",
        "    class_name = pair[\"class\"] #\"Class_name\"is equal to the pair fo the\"class\" key\n",
        "    column = pair[\"column\"] #and \"column\" is equal to the column key\n",
        "\n",
        "    scraped_data = tag_scrape(entry,tag,class_name)#this passes the entry,tag, and class_name values to our \"tag_scrape\"function\n",
        "\n",
        "    entry_data[column] = ', '.join(scraped_data) #Reorganizing dictionary based on column\n",
        "\n",
        "\n",
        "  data.append(entry_data)\n",
        "print(data)\n",
        "# Create a DataFrame from the collected data\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Display the DataFrame\n",
        "df\n"
      ],
      "metadata": {
        "id": "kJ_YuM4msne5",
        "collapsed": true,
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title B4: Multi-Tag Data Export cell\n",
        "#@markdown Run this file to export your multi-tag data.\n",
        "#def save_type(file,fileChoice):\n",
        "\n",
        "def saveFile(fileName, dataframe):\n",
        "  file_path = Path(\"/content/drive/My Drive/scraper\") / fileName\n",
        "  file_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "  df2.to_csv(file_path, index=False)\n",
        "  return file_path\n",
        "\n",
        "print(Fore.RED + \"Would you like to export your data as a CSV or TXT file?\")\n",
        "user_input = input(f\"Please enter '+' to save the whole dataframe, or '-' if you wouldlike to choose specific columns\")\n",
        "\n",
        "\n",
        "if user_input == \"+\":\n",
        "  fileChoice = input(\"would you like to save this file as a CSV or a txt file? Please input c for CSV or t for txt\").lower()\n",
        "  urFile = input(\"Now please enter the name of the file\")\n",
        "  if fileChoice == \"c\":\n",
        "    file = urFile+\".csv\"\n",
        "    file_path = saveFile(file,df)\n",
        "    print(Fore.GREEN + f\"Success! Check your drive for your CSV file for {file_path}\")\n",
        "  elif fileChoice == \"t\":\n",
        "    file = urFile+\".txt\"\n",
        "    file_path = saveFile(file,df)\n",
        "    print(Fore.GREEN + f\"Success! Check your drive for your TXT file for {file_path}\")\n",
        "  else :\n",
        "    print(\"please enter c or t\")\n",
        "\n",
        "elif user_input == \"-\":\n",
        "  columnInput = input(\"Please enter the column names you want to save, seperated by a comma\")\n",
        "  names = columnInput.split(',')\n",
        "  names = [name.strip().lower() for name in names]\n",
        "  df2 = df[names]\n",
        "  fileChoice = input(\"would you like to save this file as a CSV or a txt file? Please input c for CSV or t for txt\").lower()\n",
        "  urFile = input(\"Now please enter the name of the file\")\n",
        "\n",
        "  if fileChoice == \"c\":\n",
        "    file = urFile+\".csv\"\n",
        "    file_path = saveFile(file,df2)\n",
        "    print(Fore.GREEN + f\"Success! Check your drive for your CSV file for {file_path}\")\n",
        "\n",
        "  elif fileChoice == \"t\":\n",
        "    file = urFile+\".txt\"\n",
        "    file_path = saveFile(file,df2)\n",
        "    print(Fore.GREEN + f\"Success! Check your drive for your TXT file for {file_path}\")\n",
        "  else :\n",
        "    print(\"please enter c or t\")\n",
        "else:\n",
        "  print('please enter + or -')\n"
      ],
      "metadata": {
        "id": "6Qh9Or8uPpTm",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section C: Internal Link Crawler"
      ],
      "metadata": {
        "id": "GcfHJwtGehZh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Website Crawler and Link Collecter\n",
        "\n",
        "Do you want to get a better idea of the entirety of your chosen website? This cell will take your URL, and attempt to output a list of all internal website links\n",
        "<br><br>\n",
        " This should allow you to get an idea of the links contained in your websites, the links on each specific webpage, and the text (if any) that is conatained in the body paragraphs\n",
        "\n",
        " You can export this data at the end of this section using the \"data export\" cell\n",
        "\n",
        " My code here began as a fork of https://github.com/mujeebishaque/extract-urls/blob/main/README.md, I built on the framework provided, and I used ChatGPT to assist me when I was stuck. In particular, I had no idea I could use a *set* to comtain a list of already-scraped collected!\n"
      ],
      "metadata": {
        "id": "TFZZfdyEQzs6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### Website Crawler\n",
        "\n",
        "#@markdown 1. Find a link to the website you want to scrape and click the arrow\n",
        "#@markdown 2. Enter that link, hit enter,and wait for the crawler (it may trake up 5 to ten minutes depending on the size of the website)\n",
        "#@markdown 3. Explore the resulting Dataframe, and decide which columns you want to export to your google drive as a CSV or TXT file.\n",
        "#@markdown <br> <br>\n",
        "#@markdown This can be a great way to get a general overview of what's on your chosen website, and help you isolate any individual webpages you may want to analyze in more detail.\n",
        "#@markdown <br> If you find another page of interest, you can return back to the scraper cells, and repeat the exploratory proces\n",
        "#XML file and scraper are returning vastly different values, are some links in the XML simply no longer linked on the website?\n",
        "\n",
        "#This function takes the user input and uses urllib's URL parse to create a key-value pair for each section of the URL\n",
        "# We to extract the base URL\n",
        "\n",
        "\n",
        "def parse_url(user_input_url): #thiscell uses the \"urllib\" library to pares URL's\n",
        "    #this line of code breaks the URL\n",
        "    #into key-value pairs\n",
        "    parsed_url = urlparse(user_input_url)\n",
        "    #by returning only the \"scheme\" and \"netloc\"\n",
        "    return f\"{parsed_url.scheme}://{parsed_url.netloc}\"\n",
        "    #the code returns a base_url by reassembling the scheme and netloc\n",
        "def fetch_website_content(current_url):\n",
        "    try:\n",
        "        #pulls the HTML from the current_url\n",
        "        response = requests.get(current_url.strip())\n",
        "        #checks if the website responds\n",
        "        response.raise_for_status()\n",
        "        return response.text #this returns the text of the response to the \"collect link data\" function\n",
        "        #catches exceptions from the requests library\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Failed to fetch {current_url}: {e}\")\n",
        "        return None\n",
        "\n",
        "def categorize_urls(links, base_url):\n",
        "    internal_urls = []\n",
        "    external_urls = []\n",
        "\n",
        "    for link in links:\n",
        "        href = link.get('href')\n",
        "        if href and href != \"#\":  # Skip placeholder links\n",
        "            full_url = urljoin(base_url, href)\n",
        "            parsed_url = urlparse(full_url)\n",
        "            if not parsed_url.scheme or not parsed_url.netloc:\n",
        "                continue  # Skip malformed URLs\n",
        "            if base_url in full_url:\n",
        "                internal_urls.append(full_url)\n",
        "            else:\n",
        "                external_urls.append(full_url)\n",
        "\n",
        "    return internal_urls, external_urls\n",
        "\n",
        "def collect_link_data(current_url, base_url, visited_urls, link_data):\n",
        "    #passess the current_url to the function \"fetch_website_content\"\n",
        "    website_content = fetch_website_content(current_url)\n",
        "    if website_content is None:\n",
        "        return []\n",
        "\n",
        "    soup = BeautifulSoup(website_content, 'html.parser') #creates beautiful sooup object\n",
        "    title = soup.title.string if soup.title else 'No Title' #pulls title and categorizes it under the variable \"title\"\n",
        "    text = \" \".join([spime.text.strip() for spime in soup.find_all([\"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"p\"])]) #pulls sample text from the BeautifulSoup Object\n",
        "  #this pulls internal links from external links by passing 'a' tags pulled from the BeautifulSoup Object into the \"categorize URLS\" function\n",
        "    internal_links, external_links = categorize_urls(soup.find_all('a', href=True), base_url)\n",
        "    #appends data (including internal and external links from the categorize URLS function) into a list of dictionaries\n",
        "    link_data.append({'URL': current_url, 'title': title, 'text': text, 'internal_links': internal_links, 'external_links':external_links})\n",
        "\n",
        "    return internal_links #this returns the new internal links for the crawler to parse\n",
        "#This is the main crawler,and it proceeds first from the base-URL\n",
        "def crawl(base_url):\n",
        "    #Creates a set of visted URL's (sets can have no duplicate)\n",
        "    visited_urls = set()\n",
        "    #A dictionary to contain link_data\n",
        "    link_data = []\n",
        "    #A set named \"to_crawl\" that will populate with links\n",
        "    #from the function \"collect link data\"\n",
        "    to_crawl = {base_url}\n",
        "\n",
        "    #While the set is there\n",
        "    while to_crawl:\n",
        "        #current_url is taken randomly from the to_crawl set\n",
        "        current_url = to_crawl.pop()\n",
        "        #if it hasne't already been visited\n",
        "        if current_url not in visited_urls:\n",
        "            #Print what is being crawled\n",
        "            print(f\"Crawling: {current_url}\")  # Print the current URL so the user sees\n",
        "            #Add the URL to the visited_url Set\n",
        "            visited_urls.add(current_url)\n",
        "            #Collect link data that is stored in the dictionary\n",
        "            new_internal_links = collect_link_data(current_url, base_url, visited_urls, link_data)\n",
        "            #Update the set \"to crawl\" with the links pulledfrom \"new_links\"\n",
        "            to_crawl.update(new_internal_links)\n",
        "\n",
        "    return pd.DataFrame(link_data), pd.DataFrame(visited_urls)\n",
        "\n",
        "# The maine function\n",
        "if __name__ == '__main__':\n",
        "    #asks for input url\n",
        "    user_input_url = input(\"Input URL: \")\n",
        "    if not user_input_url:\n",
        "        raise Exception(\"INFO: Invalid Input\")\n",
        "    #passes base url to URLLIB url paresr\n",
        "    base_url = parse_url(user_input_url)\n",
        "    #starts cralwer using base URL\n",
        "    df_links, df_visited_Links = crawl(base_url)\n",
        "    #Data export\n",
        "df_links\n",
        "\n"
      ],
      "metadata": {
        "id": "i5cECfS2-u7K",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### Updated Data Export cell for link export\n",
        "#@markdown Follow the prompts to save your crawled website as a CSV or TXT file\n",
        "#@markdown <br> If you are only interested in website URL's, make sure to chose to save only specific columns, and the appropriate column name when prompted\n",
        "#def save_type(file,fileChoice):\n",
        "\n",
        "def saveFile(fileName, dataframe):\n",
        "  file_path = Path(\"/content/drive/My Drive/scraper\") / fileName\n",
        "  file_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "  df_links.to_csv(file_path, index=False)\n",
        "  return file_path\n",
        "\n",
        "print(Fore.RED + \"Would you like to export your data as a CSV or TXT file?\")\n",
        "user_input = input(f\"Please enter '+' to save the whole dataframe, or '-' if you wouldlike to choose specific columns\")\n",
        "\n",
        "\n",
        "if user_input == \"+\":\n",
        "  fileChoice = input(\"would you like to save this file as a CSV or a txt file? Please input c for CSV or t for txt\").lower()\n",
        "  urFile = input(\"Now please enter the name of the file\")\n",
        "  if fileChoice == \"c\":\n",
        "    file = urFile+\".csv\"\n",
        "    file_path = saveFile(file,df_links)\n",
        "    print(Fore.GREEN + f\"Success! Check your drive for your CSV file for {file_path}\")\n",
        "  elif fileChoice == \"t\":\n",
        "    file = urFile+\".txt\"\n",
        "    file_path = saveFile(file,df_links)\n",
        "    print(Fore.GREEN + f\"Success! Check your drive for your TXT file for {file_path}\")\n",
        "  else :\n",
        "    print(\"please enter c or t\")\n",
        "\n",
        "elif user_input == \"-\":\n",
        "  columnInput = input(\"Please enter the column names you want to save, seperated by a comma\")\n",
        "  names = columnInput.split(',')\n",
        "  names = [name.strip() for name in names]\n",
        "  df_links = df_links[names]\n",
        "  fileChoice = input(\"would you like to save this file as a CSV or a txt file? Please input c for CSV or t for txt\").lower()\n",
        "  urFile = input(\"Now please enter the name of the file\")\n",
        "\n",
        "  if fileChoice == \"c\":\n",
        "    file = urFile+\".csv\"\n",
        "    file_path = saveFile(file,df_links)\n",
        "    print(Fore.GREEN + f\"Success! Check your drive for your CSV file  '{file_path}'\")\n",
        "\n",
        "  elif fileChoice == \"t\":\n",
        "    file = urFile+\".txt\"\n",
        "    file_path = saveFile(file,df_links)\n",
        "    print(Fore.GREEN + f\"Success! Check your drive for your TXT file  '{file_path}'\")\n",
        "  else :\n",
        "    print(\"please enter c or t\")\n",
        "else:\n",
        "  print('please enter + or -')\n"
      ],
      "metadata": {
        "id": "WwGWyuSx487r",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}